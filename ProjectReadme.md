# Gesture Recogntion [ Exact Use case to be identifies later ]
Team Members:
- Om Samel, ompradee@buffalo.edu
- Chigozie Eke, cmeke@buffalo.edu

--- 

## Gesture Recogntion with a Robotic Arm
We propose to develop a gesture recognition system that detects and classifies hand gestures in real-time using computer vision. The system will serve as a foundational module for controlling robotic systems through natural human gestures. This can later be applied to robotic arms, drones, smart wheelchairs, or warehouse automation.




## Contributions
- Gesture-based control offers an **intuitive** and **hands-free** method of interacting with robotic systems.
- Unlike traditional control systems (buttons, joysticks, or voice commands), gesture recognition enables **natural** interaction.
- This project lays the groundwork for **human-robot collaboration**, **assistive robotics**, and **industrial automation**.
- The project is **scalable** and can be integrated with **ROS-based robotic systems, drones, or AR/VR interfaces** in the future.

## Project Plan
- **Online Materials:**
  - Official **Mediapipe** documentation (Google) for hand tracking  
  - OpenCV tutorials for **computer vision**  
  - Research papers on **gesture-based HRI (Human-Robot Interaction)**  
- **Textbook References:**
  - *Robotics: Modelling, Planning and Control* – Siciliano & Khatib  
  - *Computer Vision: Algorithms and Applications* – Szeliski (for OpenCV basics)  
- **Development Tools:**
  - **Python, OpenCV, Mediapipe** for gesture recognition  
  - **Jupyter Notebook/PyCharm** for development  
  - **GitHub** for documentation and collaboration  
  - *(Optional)* ROS/Gazebo for simulated robotic control 

## Milestones/Schedule Checklist
| **Task** | **Assigned To** | **Due Date** | **Status** |
|----------|---------------|-------------|------------|
| Complete this proposal document |  | **Feb 28** |Pending |
| Set up environment (install OpenCV, Mediapipe) |  | **March 5** | Pending |
| Implement real-time hand tracking |  | **March 12** | Pending |
| Develop a gesture classification model |  | **March 19** | Pending |
| Map gestures to robotic commands (simulation) |  | **March 26** | Pending |
| Create progress report |  | **April 3** | Pending |
| Test and refine gesture recognition accuracy |  | **April 10** | Pending |
| Prepare demo with a simple robotic control application |  | **April 24** | Pending |
| Create final presentation |  | **May 6** | Pending |
| Provide system documentation (README.md) |  | **May 13** | Pending |


## Measures of Success
The system should be able to **accurately detect and classify hand gestures** with at least **90% accuracy** in varied lighting conditions.  
The gestures should trigger corresponding robotic commands (e.g., **"move left" when swiping left**).  
A **demonstration (real-time video or simulation)** should show the system in action.  
Even if full implementation is not achieved, **partial credit should be given** if:  
   - Hand tracking works reliably.  
   - At least **3-5 different gestures** are correctly classified.  
   - A basic robotic system (virtual or real) can respond to at least **one** gesture-based command.  
